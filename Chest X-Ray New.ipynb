{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential, Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Input, Dense, BatchNormalization, Conv2D, Conv2DTranspose, ReLU, LeakyReLU, Flatten, MaxPooling2D, Dropout, Reshape\n",
        "\n",
        "# Setting up distributed strategy for multi-GPU training\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "print('DEVICES AVAILABLE: {}'.format(strategy.num_replicas_in_sync))\n",
        "\n",
        "# Hyperparameters\n",
        "BUFFER_SIZE = 64000\n",
        "BATCH_SIZE = 32 * strategy.num_replicas_in_sync\n",
        "EPOCHS = 50\n",
        "latent_dim = 128\n",
        "input_size = [512, 512, 3]  # Changed input size to 512x512\n",
        "image_size = (512, 512)\n",
        "\n",
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Load dataset\n",
        "image_directory = 'chest-xray-pneumonia/chest_xray'\n",
        "dataset = datagen.flow_from_directory(\n",
        "    os.path.join(image_directory, 'train'),\n",
        "    classes=['PNEUMONIA'],\n",
        "    target_size=image_size,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Define Generator Model\n",
        "def gen_model():\n",
        "    model = Sequential([\n",
        "        Input(shape=(latent_dim,)),\n",
        "        Dense(8 * 8 * 256),\n",
        "        Reshape((8, 8, 256)),\n",
        "        Conv2DTranspose(128 * 2, kernel_size=4, strides=2, padding='same'),\n",
        "        LeakyReLU(alpha=0.1),\n",
        "        Conv2DTranspose(128 * 4, kernel_size=4, strides=2, padding='same'),\n",
        "        LeakyReLU(alpha=0.1),\n",
        "        Conv2DTranspose(128 * 4, kernel_size=4, strides=2, padding='same'),\n",
        "        LeakyReLU(alpha=0.1),\n",
        "        Conv2DTranspose(128 * 8, kernel_size=4, strides=2, padding='same'),\n",
        "        LeakyReLU(alpha=0.1),\n",
        "        Conv2DTranspose(128 * 8, kernel_size=4, strides=2, padding='same'),\n",
        "        LeakyReLU(alpha=0.1),\n",
        "        Conv2D(3, kernel_size=4, padding='same', activation='sigmoid')\n",
        "    ], name=\"generator\")\n",
        "    return model\n",
        "\n",
        "# Define Discriminator Model\n",
        "def disc_model():\n",
        "    model = Sequential([\n",
        "        Input(shape=input_size),\n",
        "        Conv2D(256, kernel_size=4, strides=2, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(alpha=0.1),\n",
        "        MaxPooling2D(strides=2),\n",
        "        Conv2D(256 * 2, kernel_size=4, strides=2, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(alpha=0.1),\n",
        "        MaxPooling2D(strides=2),\n",
        "        Conv2D(256 * 4, kernel_size=4, strides=2, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(alpha=0.1),\n",
        "        MaxPooling2D(strides=2),\n",
        "        Flatten(),\n",
        "        Dense(256 * 4),\n",
        "        LeakyReLU(alpha=0.1),\n",
        "        Dropout(0.2),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ], name=\"discriminator\")\n",
        "    return model\n",
        "\n",
        "with strategy.scope():\n",
        "    generator = gen_model()\n",
        "    discriminator = disc_model()\n",
        "\n",
        "# Display model summaries\n",
        "generator.summary()\n",
        "discriminator.summary()\n",
        "\n",
        "# Helper function to load images in batches\n",
        "def image_loader(generator):\n",
        "    for images, labels in generator:\n",
        "        yield images, labels\n",
        "\n",
        "# Custom GAN Class\n",
        "class Gan(Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, disc_opt, gen_opt, loss_function):\n",
        "        super().compile()\n",
        "        self.disc_opt = disc_opt\n",
        "        self.gen_opt = gen_opt\n",
        "        self.loss_function = loss_function\n",
        "        self.disc_loss_metric = tf.keras.metrics.Mean(name=\"disc_loss\")\n",
        "        self.gen_loss_metric = tf.keras.metrics.Mean(name=\"gen_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.disc_loss_metric, self.gen_loss_metric]\n",
        "\n",
        "    # Custom training step\n",
        "    def train_step(self, data):\n",
        "        real_images, real_labels = data\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            disc_loss = self.loss_function(labels, predictions)\n",
        "\n",
        "        grads = tape.gradient(disc_loss, self.discriminator.trainable_weights)\n",
        "        self.disc_opt.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n",
        "\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            gen_loss = self.loss_function(misleading_labels, predictions)\n",
        "\n",
        "        grads = tape.gradient(gen_loss, self.generator.trainable_weights)\n",
        "        self.gen_opt.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        self.disc_loss_metric.update_state(disc_loss)\n",
        "        self.gen_loss_metric.update_state(gen_loss)\n",
        "\n",
        "        return {\n",
        "            \"disc_loss\": self.disc_loss_metric.result(),\n",
        "            \"gen_loss\": self.gen_loss_metric.result(),\n",
        "        }\n",
        "\n",
        "# Instantiate and compile the GAN\n",
        "with strategy.scope():\n",
        "    gan = Gan(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "    gan.compile(\n",
        "        disc_opt=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5),\n",
        "        gen_opt=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5),\n",
        "        loss_function=tf.keras.losses.BinaryCrossentropy(),\n",
        "    )\n",
        "\n",
        "# Callback for generating images\n",
        "class Gan_Callback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, model, latent_dim, filename='gan_generated.png'):\n",
        "        super(Gan_Callback, self).__init__()\n",
        "        self.model = model\n",
        "        self.latent_dim = latent_dim\n",
        "        self.filename = filename\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(16, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images = tf.clip_by_value(generated_images, 0.0, 1.0)\n",
        "        generated_images = generated_images.numpy() * 255\n",
        "        generated_images = generated_images.astype(np.uint8)\n",
        "\n",
        "        # Save generated images as a grid\n",
        "        fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
        "        for i, ax in enumerate(axes.flatten()):\n",
        "            ax.imshow(generated_images[i])\n",
        "            ax.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.filename)\n",
        "        plt.close(fig)\n",
        "\n",
        "# Start training the GAN\n",
        "gan_callback = Gan_Callback(model=gan, latent_dim=latent_dim)\n",
        "gan.fit(\n",
        "    image_loader(dataset),\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=len(dataset) // BATCH_SIZE,\n",
        "    callbacks=[gan_callback]\n",
        ")\n",
        "\n",
        "# Save the models\n",
        "generator.save('generator_model.h5')\n",
        "discriminator.save('discriminator_model.h5')\n",
        "print(\"Models saved successfully!\")\n",
        "\n",
        "# Calculate FID score for evaluation (optional)\n",
        "# This requires an implementation of the FID score calculation\n",
        "\n",
        "def calculate_inception_score(generator, latent_dim, n_samples=100):\n",
        "    from tensorflow.keras.applications import InceptionV3\n",
        "    from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "\n",
        "    # Load InceptionV3 model for feature extraction\n",
        "    inception_model = InceptionV3(include_top=False, pooling='avg')\n",
        "\n",
        "    # Generate fake images\n",
        "    noise = np.random.normal(0, 1, (n_samples, latent_dim))\n",
        "    fake_labels = np.random.randint(0, 2, n_samples)  # Random binary labels\n",
        "    generated_images = generator.predict([noise, fake_labels])\n",
        "\n",
        "    # Preprocess images for InceptionV3\n",
        "    generated_images_rescaled = preprocess_input(generated_images)\n",
        "    predictions = inception_model.predict(generated_images_rescaled)\n",
        "\n",
        "    # Calculate Inception Score\n",
        "    p_y = np.mean(predictions, axis=0)  # Marginal distribution\n",
        "    kl_div = predictions * (np.log(predictions + 1e-9) - np.log(p_y + 1e-9))\n",
        "    inception_score = np.exp(np.mean(np.sum(kl_div, axis=1)))\n",
        "\n",
        "    print(f'Inception Score: {inception_score}')\n",
        "    return inception_score\n",
        "\n",
        "def calculate_fid(generator, real_images, latent_dim, n_samples=100):\n",
        "    from scipy.linalg import sqrtm\n",
        "\n",
        "    # Generate fake images\n",
        "    noise = np.random.normal(0, 1, (n_samples, latent_dim))\n",
        "    fake_labels = np.random.randint(0, 2, n_samples)\n",
        "    generated_images = generator.predict([noise, fake_labels])\n",
        "\n",
        "    # Calculate mean and covariance for real and generated images\n",
        "    mu1, sigma1 = np.mean(real_images, axis=0), np.cov(real_images, rowvar=False)\n",
        "    mu2, sigma2 = np.mean(generated_images, axis=0), np.cov(generated_images, rowvar=False)\n",
        "\n",
        "    # Calculate FID\n",
        "    ssdiff = np.sum((mu1 - mu2) ** 2.0)\n",
        "    covmean = sqrtm(sigma1.dot(sigma2))\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "\n",
        "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        "    print(f'FID: {fid}')\n",
        "    return fid\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Example of a hyperparameter tuning process\n",
        "def compile_generator_with_hyperparams(latent_dim, lr, beta_1):\n",
        "    generator = build_generator(latent_dim, num_classes=2)\n",
        "    optimizer = Adam(learning_rate=lr, beta_1=beta_1)\n",
        "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    return generator\n",
        "\n",
        "# Try different hyperparameters\n",
        "generator = compile_generator_with_hyperparams(latent_dim=128, lr=0.0002, beta_1=0.5)\n",
        "\n",
        "# Save the generator and discriminator models\n",
        "generator.save('generator_model.h5')\n",
        "discriminator.save('discriminator_model.h5')\n",
        "\n",
        "# Save the entire cGAN model\n",
        "cgan.save('cgan_model.h5')\n",
        "\n",
        "# Load the models back for future use\n",
        "from tensorflow.keras.models import load_model\n",
        "loaded_generator = load_model('generator_model.h5')\n",
        "loaded_discriminator = load_model('discriminator_model.h5')\n",
        "loaded_cgan = load_model('cgan_model.h5')\n",
        "\n",
        "import datetime\n",
        "\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "# Add TensorBoard callback to training loop\n",
        "cgan.fit(dataset, epochs=50, callbacks=[tensorboard_callback])\n",
        "\n",
        "def generate_and_save_images(generator, latent_dim, class_label, output_dir='generated_images'):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    noise = np.random.normal(0, 1, (10, latent_dim))\n",
        "    labels = np.array([class_label] * 10)\n",
        "    generated_images = generator.predict([noise, labels])\n",
        "\n",
        "    for i, img in enumerate(generated_images):\n",
        "        plt.imsave(f\"{output_dir}/image_{i}.png\", img.reshape(128, 128), cmap='gray')\n",
        "\n",
        "generate_and_save_images(loaded_generator, latent_dim=128, class_label=1)"
      ],
      "metadata": {
        "id": "M7pMHXfO8VBK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}